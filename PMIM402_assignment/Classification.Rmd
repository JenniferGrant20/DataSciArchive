---
title: "PMIM402J Classification"
author: 'Student no. 2005070'
date: "2024-07-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries and setup

```{r, results = 'hide', error = FALSE, message = FALSE, warning = FALSE}
library(tidyverse) # For code syntax
library(mice) # For imputation
library(pROC) # For roc() and ggroc()
library(randomForest) # For random forest analysis
library(DataExplorer) # For plot_correlation()
library(gtsummary) # For tbl_regression()
seed <- 50
set.seed(seed)
```

## Data preparation and preprocessing

**Q1a. Did you undertake any preprocessing? If so, why?**

Preprocessing for both classifiers was undertaken after viewing a summary of the original data. The pacemaker variable was removed as it only consisted of one level and therefore would not have been useful in classification. The X variable was also removed as the numbers were random and unnecessary. The drug variable was recoded so that “none” was replaced by 0, “aspirin” or “clopidogrel” were replaced by 1, and “both” was replaced by 2, indicating the number of prescribed cardiovascular drugs. The family history variable was also recoded so that “no” was replaced by 0 and “yes” was replaced by 1. This recoding was necessary so that the data was in a suitable form for classification analysis. Histograms were then plotted which revealed that the cholesterol, maximum heart rate and resting blood pressure variables (‘chol’, ‘thalach’ and ‘trestbps’ respectively) had multiple ‘0’ values which would not be possible. As such, these values were replaced by NAs and suitable values were imputed using MICE. However, the ‘chol’ variable was then removed as over 20% of the data had been missing. A summary of the data was viewed again to check that the imputed data looked normal. Finally, factor variables were formatted as such before the data was shuffled randomly and split into train (60%) and test (40%) data frames so that the machine learning models could be trained and tested using different data.

```{r}
# Loading the data and looking at it
data <- read.csv('heart_disease_modified.csv')
summary(data)

# Removing unnecessary variables
data <- data %>% select(-X, -pace_maker, -Patient_ID)

# Recoding character variables into numeric
data$drug <- recode(data$drug, 
                    "None" = 0, "Aspirin" = 1, "Clopidogrel" = 1, "Both" = 2)

data$fam_hist <- recode(data$fam_hist, "no" = 0, "yes" = 1)

# Looking at the data using histograms
data %>% gather(attributes, value, 1:ncol(data)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = "lightblue", colour = "black") +
  facet_wrap(~attributes, scales = "free")

# Showed lots of zeroes for variables chol, thalach & trestbps which we will convert to NAs
data <- data %>% mutate(chol = ifelse(chol == 0, NA, chol),
                        thalach = ifelse(thalach == 0, NA, thalach),
                        trestbps = ifelse(trestbps == 0, NA, trestbps))

# Percentage missing for each of the variables
data %>% filter(is.na(chol)) %>% nrow() / 920 * 100
data %>% filter(is.na(thalach)) %>% nrow() / 920 * 100
data %>% filter(is.na(trestbps)) %>% nrow() / 920 * 100

# Looking at the missing data
md.pattern(data) 
```

```{r, results = 'hide', warning=FALSE}
# Imputation
set.seed(seed)
imputedata <- data
imputedata <- mice(imputedata, m = 5, method= 'pmm')
imputedata <- complete(imputedata)
```

```{r}
# Checking the data looks okay after imputation
summary(imputedata) 
# And that there is no missing data now
md.pattern(imputedata)

# Removing cholesterol variable as there were too many NAs prior to imputation (>20%)
imputedata <- imputedata %>% select(-chol) 

# Checking the data after imputation using histograms
imputedata %>% gather(attributes, value, 1:ncol(imputedata)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = "lightblue", colour = "black") +
  facet_wrap(~attributes, scales = "free")

# Formatting factor variables
imputedata$sex <- as.factor(imputedata$sex)
imputedata$cp <- as.factor(imputedata$cp)
imputedata$fbs <- as.factor(imputedata$fbs)
imputedata$restecg <- as.factor(imputedata$restecg)
imputedata$exang <- as.factor(imputedata$exang)
imputedata$slope <- as.factor(imputedata$slope)
imputedata$thal <- as.factor(imputedata$thal)
imputedata$smoker <- as.factor(imputedata$smoker)
imputedata$drug <- as.factor(imputedata$drug)
imputedata$fam_hist <- as.factor(imputedata$fam_hist)

## Splitting into train and test data
set.seed(seed)
imputedata$class <- as.factor(imputedata$class)
bound <- (nrow(imputedata)/5)*3
imputedata <- imputedata[(sample(nrow(imputedata))),] 
df.train <- imputedata[1:bound,]
df.test <- imputedata[(bound+1):nrow(imputedata),]
```

## Logistic regression

**Q1b. Run the classifier with default parameters.**

```{r}
# Training the regression model using training data
lr <- glm(class ~ ., family = "binomial", data = df.train) 
lr

# Results table
tbl_regression(lr, exponentiate = TRUE) 

# Applying the model to the test data
preds.lr <- predict(lr, df.test, type = "response")

# Creating a confusion matrix
cm.lr <- table(predicted = as.numeric(preds.lr>0.5), truth = df.test$class)
cm.lr

# Checking accuracy
paste0("The accuracy is ", round(((cm.lr[1,1] + cm.lr[2,2]) / nrow(df.test)) * 100, 2), "%.")
# Checking sensitivity
paste0("The sensitivity is ", round((cm.lr[2, 2] / (cm.lr[2, 2] + cm.lr[2, 1]) * 100), 2), "%.")
# Checking specificity
paste0("The specificity is ", round((cm.lr[1, 1] / (cm.lr[1, 1] + cm.lr[1, 2]) * 100), 2), "%.")
```

**Q1c. How accurately can the classifier predict those that develop heart disease? What is in the output that signifies this?**

The overall accuracy of logistic regression was 77.99%. This was found by comparing the predicted outcomes with the true outcomes of the test data. By summing the correctly predicted negatives and correctly predicted positives, dividing by the total number of outcomes and multiplying by 100, we produce a percentage of the overall accuracy from correct predictions. So, ((122 + 165) / 368) \* 100 = 77.99%. However, the sensitivity better informs one of how accurately the classifier predicts those that develop heart disease. Referring to values in the confusion matrix, the sensitivity can be calculated by dividing the number of correctly predicted true positives by the total number of predicted positives, and then multiplying by 100 to create a percentage. Therefore, the sensitivity is (165 / (165 + 38)) \* 100 = 81.28%.

**Q1d. How many people are misclassified as developing heart disease? Where is this answer found in the output?**

38 people were misclassified as developing heart disease. This is the lower left number in the above confusion matrix, where individuals were predicted to have heart disease but in truth did not have heart disease.

**Q1e. Plot and submit ROC curves for the class that develops heart disease. What is another measure of accuracy commonly used?**

Another common measure of accuracy is the area under the ROC curve, known as the AUC. As with accuracy percentages, the higher the number the better. For logistic regression, the AUC was 0.86.

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.lr <- roc(class ~ preds.lr, data = df.test)
res.lr$auc
ggroc(res.lr, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.lr$auc, 2)))
```

## Random forest

NB. Please refer to Q1a. above as the preprocessing was undertaken at once for both analyses.

**Q1b. Run the classifier with default parameters.**

```{r}
# Training the model
set.seed(seed)
rf <- randomForest(class ~ ., data = df.train)
plot(rf) # Plotting the model
varImpPlot(rf) # Plotting the variable importance

# Applying the algorithm to the test data
preds.rf <- predict(rf, newdata = df.test, type = "response")

# Creating a confusion matrix
cm.rf <- table(predicted = preds.rf, truth = df.test$class)
cm.rf

# Checking accuracy
paste0("The accuracy is ", round(((cm.rf[1,1] + cm.rf[2,2]) / nrow(df.test)) * 100, 2), "%.")
# Checking sensitivity
paste0("The sensitivity is ", round((cm.rf[2, 2] / (cm.rf[2, 2] + cm.rf[2, 1]) * 100), 2), "%.")
# Checking specificity
paste0("The specificity is ", round((cm.rf[1, 1] / (cm.rf[1, 1] + cm.rf[1, 2]) * 100), 2), "%.")
```

**Q1c. How accurately can the classifier predict those that develop heart disease? What is in the output that signifies this?**

The overall accuracy of the random forest was ((120 + 171) / 368) \* 100 = 79.08%. However, the accuracy of predicting those that develop heart disease, i.e. the sensitivity, was (171 / (171 + 40)) \* 100 = 81.04%. These values are drawn from the confusion matrix and the calculations are the same as were used for the logistic regression.

**Q1d. How many people are misclassified as developing heart disease? Where is this answer found in the output?**

40 people were misclassified as developing heart disease by random forest analysis. This is the lower left number in the above confusion matrix, where individuals were predicted to have heart disease but in truth did not have heart disease.

**Q1e. Plot and submit ROC curves for the class that develops heart disease. What is another measure of accuracy commonly used?**

Another measure of accuracy is the area under the ROC curve, known as the AUC. As with accuracy percentages, the higher the number the better. For random forest, the AUC was 0.79.

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.rf <- roc(class ~ as.numeric(preds.rf), data = df.test)
res.rf$auc
ggroc(res.rf, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.rf$auc, 2)))
```

**Q2a. Why did you choose this classifier over the other?**

Random forest was chosen over logistic regression as it had a greater accuracy percentage but a lower AUC - it is of interest to increase both of these but particularly the AUC. In R, random forest has more internal hyperparameters to optimise than logistic regression which will also provide a greater opportunity to tune the model and improve its accuracy.

**Q2b. Explain how this classifier works from a theoretical point of view – try to include as much detail as possible.**

Random forest is a form of supervised machine learning, which means that the input data contains a classifier variable – the desired output value – to train the model to predict the class of other data. Random forest creates a ‘forest’ of many decision trees through a randomised process, hence its name.

To generate the many decision trees which make up the ‘forest’, the algorithm utilises bootstrapping and aggregation (collectively known as bagging). Bootstrapping involves the algorithm selecting a random sample of the dataset from which each decision tree is created. The trees also use a random subset of features when splitting, and as such each tree will be trained independently and a wide variation of individual results from the trees will be created. This bootstrapping and random feature selection process means that random forest models are less sensitive to the original training data which helps to avoid overfitting, reduces correlations between trees and improves generalisability – all of which enhance the accuracy of predictions.

Aggregation refers to the process through which the individual results of all decision trees are combined, and the model’s final output is determined through majority voting, i.e. if 60% of the trees predict the class to be '0' and therefore 40% predict it to be '1', the output will be class '0'. Due to this bagging process random forest is also referred to as an ‘ensemble’ learning method.

## Optimising random forest using feature selection

**Q2c. (i) Were there any features that could be removed?**

A correlation matrix of all variables was created, and it was found that troponin and perfusion had a strong negative correlation. However, removing troponin did not increase the accuracy of the model but instead decreased the accuracy, so it was decided these correlated variables would continue to be included.
Next, a variable importance plot was created and the least important variables were dropped one by one. Removing 'fbs' first had a positive outcome, increasing the accuracy from 79.09% to 79.89%. However, removing the next variable 'fam_hist' decreased the accuracy to 79.35% (although still an improvement from the base model). It was decided that all variables except 'fbs' would be included in subsequent formulae. 

```{r}
# Plotting correlation matrix
plot_correlation(imputedata) # Troponin and perfusion are strongly correlated

# Training model without troponin 
rf.opt <- randomForest(class ~ cp + thalach + thal + oldpeak + age + exang + 
                         perfusion + trestbps + slope + sex + restecg + ca +
                         drug + smoker + fam_hist + fbs, 
                       data = df.train)
plot(rf.opt)

# Applying the algorithm to the test data
preds.opt <- predict(rf.opt, df.test, type = "response")

# Creating a confusion matrix
cm.opt <- table(predicted = preds.opt, truth = df.test$class)
cm.opt

# Checking accuracy (decreased)
paste0("The accuracy is ", round(((cm.opt[1,1] + cm.opt[2,2]) / nrow(df.test)) * 100, 2), "%.")
# Checking sensitivity
paste0("The sensitivity is ", round((cm.opt[2, 2] / (cm.opt[2, 2] + cm.opt[2, 1]) * 100), 2), "%.")
# Checking specificity
paste0("The specificity is ", round((cm.opt[1, 1] / (cm.opt[1, 1] + cm.opt[1, 2]) * 100), 2), "%." )

# Since removing troponin did not help, we will try removing variables of least importance 
varImpPlot(rf)

# First removing the least important variable according to the plot: fbs
set.seed(seed)
rf.opt <- randomForest(class ~ cp + thalach + thal + oldpeak + age + exang + 
                         perfusion + traponin + trestbps + slope + sex + restecg + 
                         ca + drug + smoker + fam_hist, 
                       data = df.train)
plot(rf.opt)

# Applying the algorithm to the test data
preds.opt <- predict(rf.opt, df.test, type = "response")

# Creating a confusion matrix
cm.opt <- table(predicted = preds.opt, truth = df.test$class)
cm.opt

# Checking accuracy (improved slightly this time)
round(((cm.opt[1,1] + cm.opt[2,2]) / nrow(df.test)) * 100, 2)  # accuracy
round((cm.opt[2, 2] / (cm.opt[2, 2] + cm.opt[2, 1]) * 100), 2) # sensitivity
round((cm.opt[1, 1] / (cm.opt[1, 1] + cm.opt[1, 2]) * 100), 2) # specificity
```

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.opt <- roc(class ~ as.numeric(preds.opt), data = df.test)
res.opt$auc
ggroc(res.opt, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.opt$auc, 2)))
```

```{r}
# Repeating the analysis again, removing fam_hist this time

rf.opt <- randomForest(class ~ cp + thalach + thal + oldpeak + age + exang + 
                         perfusion + traponin + trestbps + slope + sex + restecg + 
                         ca + drug + smoker, 
                       data = df.train)
plot(rf.opt)

# Applying the algorithm to the test data
preds.opt <- predict(rf.opt, df.test, type = "response")

# Creating a confusion matrix
cm.opt <- table(predicted = preds.opt, truth = df.test$class)
cm.opt

# Checking accuracy (it decreased this time so only fbs will be removed in future formulae)
round(((cm.opt[1,1] + cm.opt[2,2]) / nrow(df.test)) * 100, 2)  # accuracy
round((cm.opt[2, 2] / (cm.opt[2, 2] + cm.opt[2, 1]) * 100), 2) # sensitivity
round((cm.opt[1, 1] / (cm.opt[1, 1] + cm.opt[1, 2]) * 100), 2) # specificity


```

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.opt <- roc(class ~ as.numeric(preds.opt), data = df.test)
res.opt$auc
ggroc(res.opt, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.opt$auc, 2)))
```

## Optimising random forest by changing train and test data sampling

**Q2c. (ii) Did changing the way data is sampled during training/testing affect the accuracy?**

Yes, the accuracy increased from 79.89% to 84.78% when the train:test data ratio was changed from 60:40 to 80:20. This method of increasing the accuracy also led to the greatest increase in AUC from 0.79 to 0.85.

```{r}
# Changing train:test to 80:20
set.seed(seed)
bound.new <- (nrow(imputedata)/4)*3
imputedata <- imputedata[(sample(nrow(imputedata))),] 
df.train.opt <- imputedata[1:bound.new,]
df.test.opt <- imputedata[(bound.new+1):nrow(imputedata),] 

# Training the model
rf.opt2 <- randomForest(class ~ cp + thalach + thal + oldpeak + age + exang + 
                         perfusion + traponin + trestbps + slope + sex + restecg + 
                         ca + drug + smoker + fam_hist, 
                        data = df.train.opt)
plot(rf.opt2)
varImpPlot(rf.opt2)

# Applying the algorithm to the test data
preds.opt2 <- predict(rf.opt2, df.test.opt, type = "response")

# Creating a confusion matrix
cm.opt2 <- table(predicted = preds.opt2, truth = df.test.opt$class)
cm.opt2

# Checking accuracy
paste0("The accuracy is ", round(((cm.opt2[1,1] + cm.opt2[2,2]) / nrow(df.test.opt)) * 100, 2), "%.")
# Checking sensitivity
paste0("The sensitivity is ", round((cm.opt2[2, 2] / (cm.opt2[2, 2] + cm.opt2[2, 1]) * 100), 2), "%.")
# Checking specificity
paste0("The specificity is ", round((cm.opt2[1, 1] / (cm.opt2[1, 1] + cm.opt2[1, 2]) * 100), 2), "%.")
```

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.opt2 <- roc(class ~ as.numeric(preds.opt2), data = df.test.opt)
res.opt2$auc
ggroc(res.opt2, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.opt2$auc, 2)))
```

## Optimising random forest using hyperparameter tuning

**Q2c. (iii) What about some of the internal parameters specific to the classifier? Please explain how one of these parameters can affect accuracy.**

Optimising the mtry and ntree parameters (from default to 4 and 511 respectively) increased the accuracy from 84.78% to 85.65%, and the AUC from 0.85 to 0.86. 
Mtry is the number of variables randomly sampled at each split, whilst ntree is the number of trees to grow. Accuracy typically increases as the number of trees (ntree) increases because the results created from the randomised trees are averaged, reducing variance and improving accuracy/generalisability. However, after a certain number of trees the increase in accuracy becomes negligible. At this point it is not worth creating more trees as the increase in computational cost is not worthwhile. Looking at error plots can help to inform the optimal number of trees which will maximise accuracy and minimise computational cost. 

```{r}
# Searching for optimal mtry value
set.seed(seed)
ytrain <- df.train.opt$class
ytest <- df.test.opt$class
xtrain <- df.train.opt %>% select(-class)
xtest <- df.test.opt %>% select(-class)
bestmtry <- tuneRF(xtrain, ytrain, stepFactor = 1.5, improve = 1e-5, ntree = 511)
print(bestmtry)

# Training the model
rf.opt3 <- randomForest(class ~ cp + thalach + thal + oldpeak + age + exang + 
                         perfusion + traponin + trestbps + slope + sex + restecg + 
                         ca + drug + smoker + fam_hist, data = df.train.opt,
                        mtry = 4, ntree = 511)

# Ntree = 511 is a good value as the line on the error plot has flattened out
plot(rf.opt3)

# Applying the algorithm to the test data
preds.opt3 <- predict(rf.opt3, df.test.opt, type = "response")

# Creating a confusion matrix
cm.opt3 <- table(predicted = preds.opt3, truth = df.test.opt$class)
cm.opt3

# Checking accuracy
paste0("The accuracy is ", round(((cm.opt3[1,1] + cm.opt3[2,2]) / nrow(df.test.opt)) * 100, 2), "%.")
# Checking sensitivity
paste0("The sensitivity is ", round((cm.opt3[2, 2] / (cm.opt3[2, 2] + cm.opt3[2, 1]) * 100), 2), "%.")
# Checking specificity
paste0("The specificity is ", round((cm.opt3[1, 1] / (cm.opt3[1, 1] + cm.opt3[1, 2]) * 100), 2), "%.")
```

```{r, message=FALSE, warning=FALSE}
# ROC analysis
res.opt3 <- roc(class ~ as.numeric(preds.opt3), data = df.test.opt)
res.opt3$auc
ggroc(res.opt3, legacy.axes = TRUE) +
  labs(title = paste0("AUC = ", round(res.opt3$auc, 2)))
```

**Q2d. In general, a classifier is only as good as the data it is trained on. Please comment on what is needed from training data to train a good classifier. How can utilizing classifiers help feed back into healthcare settings with regards to data collection?**

The most important aspect of training data used for classification is that it must be accurately labelled so that the model is actually being trained to classify the data into the correct categories. The data should also be diverse and representative so that the model is effective in classifying future real-world data. Larger training datasets (e.g. 1000 rows rather than 100) typically create models with better accuracy as there are a wider range of scenarios captured, linking to this need for diversity and representativeness. It should also be ensured that training data is clean before being used to train a model, as missing or messy data would produce a greater number of incorrect predictions and therefore a less accurate model. 

Utilising classifiers can feed back into healthcare settings' data collection by streamlining the data collection process, reducing the time and resources (such as computational cost) spent. Using the example of heart disease, classifiers can help discover correlations between certain variables or determine which are the most important predictors of developing heart disease - this means collection of these data can be prioritised in an environment which is often hectic and which otherwise may have been left out, evident in how NAs are not uncommon amongst healthcare data. 
